{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraies\n"
      ],
      "metadata": {
        "id": "jv3A9oHtRPkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPX46S0yRLpS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------\n",
        "# تحميل وتنظيف الداتا\n",
        "# -------------------------\n",
        "df = pd.read_csv(\"penguins.csv\")\n",
        "df['CulmenLength']=df['CulmenLength'].fillna(df['CulmenLength'].mean())\n",
        "df['CulmenDepth']=df['CulmenDepth'].fillna(df['CulmenDepth'].mean())\n",
        "df['BodyMass']=df['BodyMass'].fillna(df['BodyMass'].mean())\n",
        "df['FlipperLength']=df['FlipperLength'].fillna(df['FlipperLength'].mean())\n",
        "# ترميز العمود النصي OriginLocation إلى أرقام\n",
        "le = LabelEncoder()\n",
        "df[\"OriginLocation\"] = le.fit_transform(df[\"OriginLocation\"])\n",
        "df.head(51)\n",
        "\n",
        "# أول 150 صف (3 classes × 50 صف لكل كلاس)\n",
        "df = df.iloc[:150].reset_index(drop=True)\n",
        "\n",
        "# تحديد رقم لكل class (0,1,2)\n",
        "df[\"Target\"] = [0]*50 + [1]*50 + [2]*50\n",
        "\n",
        "# -------------------------\n",
        "# أسماء الـ features\n",
        "# -------------------------\n",
        "feature_names = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\", \"OriginLocation\"]\n",
        "\n",
        "# -------------------------\n",
        "# signum & perceptron\n",
        "# -------------------------\n",
        "def signum(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "def perceptron_train(X, T, eta=0.01, epochs=20):\n",
        "    n_samples, n_features = X.shape\n",
        "    W = np.random.randn(n_features)\n",
        "    b = np.random.randn()\n",
        "    for _ in range(epochs):\n",
        "        for i in range(n_samples):\n",
        "            y_i = signum(np.dot(W, X[i]) + b)\n",
        "            if y_i != T[i]:\n",
        "                W += eta * (T[i] - y_i) * X[i]\n",
        "                b += eta * (T[i] - y_i)\n",
        "    return W, b\n",
        "\n",
        "# -------------------------\n",
        "# جميع تركيبات الميزات (5 choose 2 = 10)\n",
        "# -------------------------\n",
        "feature_pairs = list(itertools.combinations(feature_names, 2))\n",
        "\n",
        "# -------------------------\n",
        "# ازواج الكلاسات (0-1, 0-2, 1-2)\n",
        "# -------------------------\n",
        "class_pairs = list(itertools.combinations([0,1,2], 2))\n",
        "\n",
        "for (c1, c2) in class_pairs:\n",
        "    subset = df[(df[\"Target\"] == c1) | (df[\"Target\"] == c2)]\n",
        "    print(f\"\\nDataset: class_{c1}_{c2}\")\n",
        "\n",
        "    # نختار 30 تدريب و20 اختبار من كل كلاس عشوائيًا\n",
        "    train_c1 = subset[subset[\"Target\"] == c1].sample(n=30, random_state=42)\n",
        "    test_c1  = subset[subset[\"Target\"] == c1].drop(train_c1.index)\n",
        "    train_c2 = subset[subset[\"Target\"] == c2].sample(n=30, random_state=42)\n",
        "    test_c2  = subset[subset[\"Target\"] == c2].drop(train_c2.index)\n",
        "\n",
        "    train_df = pd.concat([train_c1, train_c2]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    test_df  = pd.concat([test_c1, test_c2]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # نحول التارجت لـ +1 و -1\n",
        "    train_df[\"Target\"] = train_df[\"Target\"].apply(lambda x: 1 if x == c1 else -1)\n",
        "    test_df[\"Target\"]  = test_df[\"Target\"].apply(lambda x: 1 if x == c1 else -1)\n",
        "\n",
        "    # -------------------------\n",
        "    # تجربة كل زوج features\n",
        "    # -------------------------\n",
        "    for f1, f2 in feature_pairs:\n",
        "        X_train = train_df[[f1, f2]].values.astype(float)\n",
        "        T_train = train_df[\"Target\"].values\n",
        "        X_test  = test_df[[f1, f2]].values.astype(float)\n",
        "        T_test  = test_df[\"Target\"].values\n",
        "\n",
        "        W, b = perceptron_train(X_train, T_train, eta=0.01, epochs=10)\n",
        "\n",
        "        preds = np.array([signum(np.dot(W, x_i) + b) for x_i in X_test])\n",
        "        acc = np.mean(preds == T_test)\n",
        "\n",
        "        print(f\"   → Features: ({f1}, {f2}) | Test Accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "i_PFm3XMh_LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read CVS"
      ],
      "metadata": {
        "id": "X01JwglTRXsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame = pd.read_csv(\"penguins.csv\")"
      ],
      "metadata": {
        "id": "f9Lh1KODRMzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# print first five rows of data"
      ],
      "metadata": {
        "id": "rfltdmdBRdgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.head(51)"
      ],
      "metadata": {
        "id": "fGjP4LW1RWZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.tail()"
      ],
      "metadata": {
        "id": "ND17OhAoR8XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.info()"
      ],
      "metadata": {
        "id": "N7Y_bVAQRz2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.describe()"
      ],
      "metadata": {
        "id": "U1owr6Q0ROBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.shape"
      ],
      "metadata": {
        "id": "CgKkyrg5a17j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.isnull().sum()"
      ],
      "metadata": {
        "id": "bGct-83kSG2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame['CulmenLength']=dataFrame['CulmenLength'].fillna(dataFrame['CulmenLength'].mean())\n",
        "dataFrame['CulmenDepth']=dataFrame['CulmenDepth'].fillna(dataFrame['CulmenDepth'].mean())\n",
        "dataFrame['BodyMass']=dataFrame['BodyMass'].fillna(dataFrame['BodyMass'].mean())\n",
        "dataFrame['FlipperLength']=dataFrame['FlipperLength'].fillna(dataFrame['FlipperLength'].mean())"
      ],
      "metadata": {
        "id": "I6fZses4SRzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.isnull().sum()"
      ],
      "metadata": {
        "id": "vOF7cjIWSh-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "dataFrame['OriginLocation_Encoded'] = le.fit_transform(dataFrame['OriginLocation'])\n",
        "print(dataFrame[['OriginLocation', 'OriginLocation_Encoded']])"
      ],
      "metadata": {
        "id": "-cL59OqKSnn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron"
      ],
      "metadata": {
        "id": "I_71FHBBdDtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame = pd.read_csv(\"penguins.csv\")\n",
        "dataFrame['CulmenLength']=dataFrame['CulmenLength'].fillna(dataFrame['CulmenLength'].mean())\n",
        "dataFrame['CulmenDepth']=dataFrame['CulmenDepth'].fillna(dataFrame['CulmenDepth'].mean())\n",
        "dataFrame['BodyMass']=dataFrame['BodyMass'].fillna(dataFrame['BodyMass'].mean())\n",
        "dataFrame['FlipperLength']=dataFrame['FlipperLength'].fillna(dataFrame['FlipperLength'].mean())\n",
        "columns = ['CulmenLength', 'CulmenDepth', 'BodyMass', 'FlipperLength']\n",
        "for i in columns:\n",
        "    q1 = np.percentile(dataFrame[i], 25)\n",
        "    q3 = np.percentile(dataFrame[i], 75)\n",
        "    norm_range = (q3 - q1) * 1.5\n",
        "    # Identify lower outliers\n",
        "    lower_outliers = dataFrame[dataFrame[i] < (q1 - norm_range)]\n",
        "    # Identify upper outliers\n",
        "    upper_outliers = dataFrame[dataFrame[i] > (q3 + norm_range)]\n",
        "    # Count the total number of outliers\n",
        "    outliers = len(lower_outliers) + len(upper_outliers)\n",
        "    print(f\"The number of outliers in {i}: {outliers}\")\n",
        "    # Replace outliers with adjusted values\n",
        "    dataFrame[i] = np.where(dataFrame[i] < (q1 - norm_range), q1 - norm_range, dataFrame[i])\n",
        "    dataFrame[i] = np.where(dataFrame[i] > (q3 + norm_range), q3 + norm_range, dataFrame[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPlwrCkdU8gx",
        "outputId": "fa8d831a-cb60-49cf-fe47-77b5fcb7ce80"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of outliers in CulmenLength: 0\n",
            "The number of outliers in CulmenDepth: 0\n",
            "The number of outliers in BodyMass: 0\n",
            "The number of outliers in FlipperLength: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# تحميل البيانات\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"penguins.csv\")\n",
        "\n",
        "# نختار فئتين فقط (Gentoo و Adelie)\n",
        "df = df[df[\"Species\"].isin([\"Gentoo\", \"Adelie\"])]\n",
        "\n",
        "# نحذف أي صف فيه بيانات ناقصة\n",
        "# df = df.dropna(subset=[\"CulmenLength\", \"FlipperLength\", \"Species\"])\n",
        "\n",
        "# نحول الفئة إلى أرقام: Gentoo = +1, Adelie = -1\n",
        "df[\"Target\"] = df[\"Species\"].apply(lambda x: 1 if x == \"Gentoo\" else -1)\n",
        "\n",
        "# ----------------------------\n",
        "# تقسيم البيانات: 30 عشوائي من كل كلاس تدريب، والباقي اختبار\n",
        "# ----------------------------\n",
        "class1 = df[df[\"Target\"] == 1]\n",
        "class2 = df[df[\"Target\"] == -1]\n",
        "\n",
        "# 30 عشوائي من كل كلاس\n",
        "train_class1 = class1.sample(n=30, random_state=42)\n",
        "test_class1  = class1.drop(train_class1.index)\n",
        "\n",
        "train_class2 = class2.sample(n=30, random_state=42)\n",
        "test_class2  = class2.drop(train_class2.index)\n",
        "\n",
        "# دمج الكلاسات في train و test\n",
        "train_df = pd.concat([train_class1, train_class2]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df  = pd.concat([test_class1, test_class2]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# فصل الـ Features والـ Target\n",
        "X_train = train_df[[\"CulmenLength\", \"FlipperLength\"]].values\n",
        "T_train = train_df[\"Target\"].values\n",
        "\n",
        "X_test = test_df[[\"CulmenLength\", \"FlipperLength\"]].values\n",
        "T_test = test_df[\"Target\"].values\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Test size:\", len(X_test))\n",
        "\n",
        "# ----------------------------\n",
        "# دالة signum\n",
        "# ----------------------------\n",
        "def signum(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "# ----------------------------\n",
        "# خوارزمية Perceptron\n",
        "# ----------------------------\n",
        "def perceptron_train(X, T, eta=0.01, epochs=20):\n",
        "    n_samples, n_features = X.shape\n",
        "    W = np.random.randn(n_features)\n",
        "    b = np.random.randn()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(n_samples):\n",
        "            x_i = X[i]\n",
        "            t_i = T[i]\n",
        "            y_i = signum(np.dot(W, x_i) + b)\n",
        "            if y_i != t_i:\n",
        "                L = (t_i - y_i)\n",
        "                W = W + eta * L * x_i\n",
        "                b = b + eta * L\n",
        "    return W, b\n",
        "\n",
        "# ----------------------------\n",
        "# تدريب النموذج\n",
        "# ----------------------------\n",
        "W, b = perceptron_train(X_train, T_train, eta=0.01, epochs=10)\n",
        "\n",
        "# ----------------------------\n",
        "# تقييم النموذج\n",
        "# ----------------------------\n",
        "pred_train = np.array([signum(np.dot(W, x_i) + b) for x_i in X_train])\n",
        "pred_test = np.array([signum(np.dot(W, x_i) + b) for x_i in X_test])\n",
        "\n",
        "train_acc = np.mean(pred_train == T_train)\n",
        "test_acc = np.mean(pred_test == T_test)\n",
        "\n",
        "print(\"\\nFinal Weights:\", W)\n",
        "print(\"Final Bias:\", b)\n",
        "print(f\"Train Accuracy: {train_acc * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "name1='Adelie'\n",
        "name2='Gentoo'\n",
        "df_1 = df.iloc[:100]\n",
        "# 2️⃣ آخر 100 صف\n",
        "df_2 = df.iloc[50:]\n",
        "if ( (name1==\"Adelie\" ) and (name2==\"Gentoo\")):\n",
        "  df_3 = pd.concat([df.iloc[:50], df.iloc[100:]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HGFjuEJGccum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataFrame['Species']\n",
        "numberOfColumns = len(dataFrame.columns)-2\n",
        "for i in range(numberOfColumns):\n",
        "    for j in range(i+1,numberOfColumns):\n",
        "        X = dataFrame.iloc[:, [i, j]]\n",
        "        print(f\"Combination: ({i}, {j})\")\n",
        "        print(X.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "SY-PxSZ-T_TO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}